{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#: imports, nothing to see here\n",
    "import random\n",
    "from collections import defaultdict, namedtuple, deque\n",
    "from itertools import product, starmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from matplotlib import animation\n",
    "import copy\n",
    "\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from numpy import sin, cos, pi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(theta=0.017310488285557382, theta_d=-0.029708126746085296, x=-0.027529462365079839, x_d=0.013533486259850888)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "State = namedtuple('State', ['theta', 'theta_d', 'x', 'x_d'])\n",
    "random = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "state = State(*random)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "        self.grav = 9.81\n",
    "        self.mass_cart = 1.0\n",
    "        self.mass_pole = 0.1\n",
    "        self.mass_total = self.mass_cart + self.mass_pole\n",
    "        self.pole_mcenter = 0.5\n",
    "        self.polemass_mom = self.mass_pole * self.pole_mcenter\n",
    "        self.force_mag = 10.0\n",
    "        self.delta_t = 0.02\n",
    "        self.lim_theta = pi / 15\n",
    "        self.start_state = self.start_state()\n",
    "        self.lim_x = 2.4\n",
    "        self.n_actions = 2\n",
    "        self.actions = [-1, 1]\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        x, theta = state[:2]\n",
    "        if (abs(state.x) >= self.lim_x) or (abs(state.theta) >= self.lim_theta):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def reward(self, state):\n",
    "        if self.is_terminal(state) is True:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def new_state(self, state, action):\n",
    "        move = self.actions[action]\n",
    "        theta, theta_d, x, x_d = state\n",
    "        F = self.force_mag * move\n",
    "        costheta = cos(theta)\n",
    "        sintheta = sin(theta)\n",
    "        temp = (F + self.polemass_mom * theta_d * theta_d * sintheta\n",
    "                ) / self.mass_total\n",
    "\n",
    "        theta_d_d = ((self.grav * sintheta - costheta * temp) /\n",
    "                         (self.pole_mcenter *\n",
    "                          (4.0 / 3.0 - self.mass_pole * costheta * costheta /\n",
    "                           self.mass_total)))\n",
    "\n",
    "        x_d_d = temp - self.polemass_mom * theta_d_d * costheta / self.mass_total\n",
    "        x_new = x + self.delta_t * x_dot\n",
    "        x_d_new = x_d + self.delta_t * x_d_d\n",
    "        theta_new = theta + self.delta_t * theta_d\n",
    "        theta_d_new = theta_d + self.delta_t * theta_d_d\n",
    "        newstate = State(theta_new, theta_d_new, x_new, x_d_new)\n",
    "        return newstate\n",
    "\n",
    "    def start_state(self):\n",
    "        random = np.random.uniform(low=-0.05, high=0.05, size=(4, ))\n",
    "        start_state = State(*random)\n",
    "        return start_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_actions = 2\n",
    "dim_states = 4\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    rms = RMSprop()\n",
    "    model.add(Dense(32, input_shape=(dim_states,), init='zero', bias=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32, init='zero', bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32, init='zero', bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(dim_actions, init='zero',bias=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('linear'))\n",
    "    return model\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 model,\n",
    "                 alpha=0.001,\n",
    "                 epsilon=0-99,\n",
    "                 gamma=1,\n",
    "                 buffer_size=300,\n",
    "                 batch_size=36):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reward_total = 0\n",
    "        self.Q = make_model()\n",
    "        self.Q_target = make_model()\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.batch_size = batch_size\n",
    "        self.actions = [-1, 1]\n",
    "        self.env = env\n",
    "        self.log = []\n",
    "        self.log_sum = []\n",
    "        self.step_count=0\n",
    "        self.episode_count=0\n",
    "        self.lern_start = 500\n",
    "        self.target_update = 500\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action(state)\n",
    "\n",
    "    def make_batch(self):\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            for state, action, newstate, reward in batch:\n",
    "                s = np.asarray(state).reshape(1,4)\n",
    "                X_train.append(s)\n",
    "                if self.step_count <= self.target_update:\n",
    "                    y = self.Q_target.predict(s)[0]\n",
    "                else:\n",
    "                    y = self.Q_target.predict(s)[0]\n",
    "                if reward == 0:\n",
    "                    y[action] = reward\n",
    "                    y_train.append(y)\n",
    "                else:\n",
    "                    ns = np.asarray(newstate).reshape(1,4)\n",
    "                    if self.step_count <= self.target_update:\n",
    "                        Q_sa = np.max(self.Q.predict(ns)[0])\n",
    "                    else:\n",
    "                        Q_sa = np.max(self.Q_target.predict(ns)[0])\n",
    "                    y[action] = reward + self.gamma * Q_sa\n",
    "                    y_train.append(y)\n",
    "            return np.vstack(X_train), np.vstack(y_train)\n",
    "\n",
    "    def flashback(self):\n",
    "        if self.step_count >= self.lern_start:\n",
    "            if len(self.memory)>= self.batch_size:\n",
    "                X, y = self.make_batch()\n",
    "                loss = self.Q.fit(X, y, verbose=0)\n",
    "        if self.step_count%self.target_update ==0:\n",
    "            self.update_target_network()\n",
    "    def update_target_network(self):\n",
    "        print('Update Target!')\n",
    "        weights = self.Q.get_weights()\n",
    "        self.Q_target.set_weights(weights)\n",
    "\n",
    "\n",
    "    def random_action(self, state):\n",
    "        choice = np.random.randint(0,2)\n",
    "        return choice\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        s = np.asarray(state).reshape(1,4)\n",
    "        Qs = self.Q.predict(s)[0]\n",
    "        action = np.argmax(Qs)\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, newstate, reward):\n",
    "        self.step_count+=1\n",
    "        if reward ==0:    \n",
    "            self.episode_count+=1\n",
    "        self.memory.append((state, action, newstate, reward))\n",
    "        self.log.append((state[0], state[1]))\n",
    "        if reward == 0:\n",
    "            self.log_sum.append(self.log)\n",
    "            self.log = []\n",
    "\n",
    "    def save_net(self):\n",
    "        self.Q.save('backup.h5')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(domain, agent):\n",
    "    state = domain.compute_start_state()\n",
    "    step = 0\n",
    "    while not domain.is_terminal(state):\n",
    "        action = agent.act(state)    #: Take the current state as input and compute an action.\n",
    "        newstate = domain.newstate(state, action)   #: Take the action and compute the changed state.\n",
    "        reward = domain.reward(newstate)\n",
    "        agent.remember(state, action, newstate, reward)#: Learn.\n",
    "        agent.flashback()\n",
    "        state = newstate                            #: Newstate becomes the current state for next iteration.\n",
    "        step +=1\n",
    "    print(step)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(domain, agent, epsilon_decay, n_episodes):\n",
    "    run_random(domain, agent)\n",
    "    for i in range(n_episodes):\n",
    "        agent.epsilon *= epsilon_decay\n",
    "        run_episode(domain, agent)\n",
    "    print('Setting epsilon paramter to zero',\n",
    "          'to prevent random actions and evaluate learned policy.\\n')\n",
    "    agent.epsilon = 0\n",
    "    run_episode(domain, agent)                    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0c63ce83bcbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepsilon_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.995\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdomain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCartPole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-2d400abaf1bf>\u001b[0m in \u001b[0;36mmake_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mrms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zero'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "epsilon_decay = 0.995\n",
    "domain = CartPole()\n",
    "model = make_model()\n",
    "agent = QAgent(domain, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
