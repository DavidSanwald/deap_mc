{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "#: imports, nothing to see here\n",
    "import random\n",
    "from collections import defaultdict, namedtuple, deque\n",
    "from itertools import product, starmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from matplotlib import animation\n",
    "import copy\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from numpy import sin, cos, pi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['theta', 'theta_d', 'x', 'x_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_actions = 2\n",
    "dim_states = 4\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    rms = RMSprop()\n",
    "    model.add(Dense(32, input_shape=(dim_states,), init='zero', bias=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32, init='zero', bias=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32, init='zero', bias=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(dim_actions, init='zero',bias=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "        self.grav = 9.81\n",
    "        self.mass_cart = 1.0\n",
    "        self.mass_pole = 0.1\n",
    "        self.mass_total = self.mass_cart + self.mass_pole\n",
    "        self.pole_mcenter = 0.5\n",
    "        self.polemass_mom = self.mass_pole * self.pole_mcenter\n",
    "        self.force_mag = 10.0\n",
    "        self.delta_t = 0.02\n",
    "        self.lim_theta = pi / 15\n",
    "        self.start_state = self.compute_start_state()\n",
    "        self.range_theta_rad = 12 * 2 * pi / 360\n",
    "        self.lim_x = 2.4\n",
    "        self.n_actions = 2\n",
    "        self.actions = [-1, 1]\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        x, theta = state[:2]\n",
    "        if (abs(x) >= self.lim_x) or (abs(theta) >= self.lim_theta):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def reward(self, state):\n",
    "        if self.is_terminal(state) is True:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def newstate(self, state, action):\n",
    "        move = self.actions[action]\n",
    "        x, theta, x_dot, theta_dot = state\n",
    "        F = self.force_mag * move\n",
    "        costheta = cos(theta)\n",
    "        sintheta = sin(theta)\n",
    "        temp = (F + self.polemass_mom * theta_dot * theta_dot *sintheta)/ self.mass_total\n",
    "\n",
    "        theta_dot_dot = (self.grav * sintheta - costheta* temp) / (self.pole_mcenter * (4.0/3.0 - self.mass_pole * costheta * costheta / self.mass_total))\n",
    "\n",
    "        x_dot_dot = temp - self.polemass_mom * theta_dot_dot * costheta /self.mass_total\n",
    "        x_new = x + self.delta_t * x_dot\n",
    "        x_dot_new = x_dot + self.delta_t * x_dot_dot\n",
    "        theta_new = theta + self.delta_t * theta_dot\n",
    "        theta_dot_new = theta_dot + self.delta_t * theta_dot_dot\n",
    "        return (x_new, theta_new, x_dot_new, theta_dot_new)\n",
    "\n",
    "    def compute_start_state(self):\n",
    "        random = np.random.uniform(low=-0.05, high=0.05, size=(4, ))\n",
    "        start_state = State(*random)\n",
    "        return start_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_actions = 2\n",
    "dim_states = 4\n",
    "\n",
    "\n",
    "def make_model(hidden_size = 64, dim_states = 4):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(dim_states,), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "    return model\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, explore=0.1, discount=0.9, hidden_size=64, memory_limit=8000):\n",
    "   \n",
    "        self.Q = make_model()\n",
    "        self.Q_target = make_model()\n",
    "        self.batch_size = 32\n",
    "        self.step_count=0\n",
    "        self.target_switch = False\n",
    "        self.target_update = 2000\n",
    "\n",
    "\n",
    "        # experience replay:\n",
    "        # remember states to \"reflect\" on later\n",
    "        self.memory = deque([], maxlen=memory_limit)\n",
    "\n",
    "        self.explore = explore\n",
    "        self.discount = discount\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.explore:\n",
    "            return np.random.randint(0, 2)\n",
    "        s = np.asarray(state).reshape(1,4)\n",
    "        q = self.Q.predict(s)\n",
    "        choice = np.argmax(q[0])\n",
    "        return choice\n",
    "\n",
    "    def remember(self, state, action, next_state, reward):\n",
    "        # the deque object will automatically keep a fixed length\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def _prep_batch(self, batch_size):\n",
    "        self.step_count+=1\n",
    "        if batch_size > self.memory.maxlen:\n",
    "            Warning('batch size should not be larger than max memory size. Setting batch size to memory size')\n",
    "            batch_size = self.memory.maxlen\n",
    "\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        # prep the batch\n",
    "        # inputs are states, outputs are values over actions\n",
    "        batch = random.sample(list(self.memory), batch_size)\n",
    "        random.shuffle(batch)\n",
    "        for state, action, next_state, reward in batch:\n",
    "            inputs.append(state)\n",
    "            s = np.asarray(state).reshape(1,4)\n",
    "            if self.target_switch:\n",
    "                target = self.Q_target.predict(s)[0]\n",
    "            else:\n",
    "                target = self.Q.predict(s)[0]\n",
    "            # debug, \"this should never happen\"\n",
    "            assert not np.array_equal(state, next_state)\n",
    "\n",
    "            # non-zero reward indicates terminal state\n",
    "            if reward == 0:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                ns = np.asarray(next_state).reshape(1,4)\n",
    "                if self.target_switch:\n",
    "                    Q_sa = np.max(self.Q_target.predict(ns)[0])\n",
    "                else:\n",
    "                    Q_sa = np.max(self.Q.predict(ns)[0])\n",
    "                target[action] = reward + self.discount * Q_sa\n",
    "            targets.append(target)\n",
    "\n",
    "        # to numpy matrices\n",
    "        return np.vstack(inputs), np.vstack(targets)\n",
    "\n",
    "    def flashback(self):\n",
    "        inputs, targets = self._prep_batch(self.batch_size)\n",
    "        loss = self.Q.train_on_batch(inputs, targets)\n",
    "        if self.step_count% self.target_update==0:\n",
    "            self.update_target_network()\n",
    "        pass\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_switch = True\n",
    "        weights = self.Q.get_weights()\n",
    "        self.Q_target.set_weights(weights)\n",
    "        pass\n",
    "\n",
    "    def save(self, fname):\n",
    "        self.Q.save_weights(fname)\n",
    "\n",
    "    def load(self, fname):\n",
    "        self.Q.load_weights(fname)\n",
    "        print(self.Q.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(domain, agent):\n",
    "    state = domain.compute_start_state()\n",
    "    step = 0\n",
    "    while not domain.is_terminal(state):\n",
    "        action = agent.act(state)    #: Take the current state as input and compute an action.\n",
    "        newstate = domain.newstate(state, action)   #: Take the action and compute the changed state.\n",
    "        reward = domain.reward(newstate)\n",
    "        agent.remember(state, action, newstate, reward)#: Learn.\n",
    "        agent.flashback()\n",
    "        state = newstate                            #: Newstate becomes the current state for next iteration.\n",
    "        step +=1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(domain, agent, epsilon_decay, n_episodes):\n",
    "    for i in range(n_episodes):\n",
    "       # agent.epsilon *= epsilon_decay\n",
    "        step = run_episode(domain, agent)\n",
    "        if step > 200:\n",
    "            print('Episode Number: {0}\\n'.format(i))\n",
    "            print('Number of steps reached: {0}\\n'.format(step))\n",
    "    print('Setting epsilon paramter to zero',\n",
    "          'to prevent random actions and evaluate learned policy.\\n')\n",
    "    # agent.epsilon = 0\n",
    "    run_episode(domain, agent)                    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "epsilon_decay = 0.9\n",
    "domain = CartPole()\n",
    "model = make_model()\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 90\n",
      "\n",
      "Number of steps reached: 366\n",
      "\n",
      "Episode Number: 128\n",
      "\n",
      "Number of steps reached: 232\n",
      "\n",
      "Episode Number: 131\n",
      "\n",
      "Number of steps reached: 203\n",
      "\n",
      "Episode Number: 135\n",
      "\n",
      "Number of steps reached: 253\n",
      "\n",
      "Episode Number: 140\n",
      "\n",
      "Number of steps reached: 212\n",
      "\n",
      "Episode Number: 157\n",
      "\n",
      "Number of steps reached: 249\n",
      "\n",
      "Episode Number: 158\n",
      "\n",
      "Number of steps reached: 219\n",
      "\n",
      "Episode Number: 173\n",
      "\n",
      "Number of steps reached: 275\n",
      "\n",
      "Episode Number: 213\n",
      "\n",
      "Number of steps reached: 246\n",
      "\n",
      "Episode Number: 216\n",
      "\n",
      "Number of steps reached: 213\n",
      "\n",
      "Episode Number: 220\n",
      "\n",
      "Number of steps reached: 259\n",
      "\n",
      "Episode Number: 237\n",
      "\n",
      "Number of steps reached: 224\n",
      "\n",
      "Episode Number: 238\n",
      "\n",
      "Number of steps reached: 202\n",
      "\n",
      "Episode Number: 240\n",
      "\n",
      "Number of steps reached: 243\n",
      "\n",
      "Episode Number: 248\n",
      "\n",
      "Number of steps reached: 201\n",
      "\n",
      "Episode Number: 249\n",
      "\n",
      "Number of steps reached: 213\n",
      "\n",
      "Episode Number: 251\n",
      "\n",
      "Number of steps reached: 339\n",
      "\n",
      "Episode Number: 252\n",
      "\n",
      "Number of steps reached: 232\n",
      "\n",
      "Episode Number: 263\n",
      "\n",
      "Number of steps reached: 261\n",
      "\n",
      "Episode Number: 264\n",
      "\n",
      "Number of steps reached: 256\n",
      "\n",
      "Episode Number: 269\n",
      "\n",
      "Number of steps reached: 260\n",
      "\n",
      "Episode Number: 281\n",
      "\n",
      "Number of steps reached: 209\n",
      "\n",
      "Episode Number: 284\n",
      "\n",
      "Number of steps reached: 220\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-d87017155875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-256-94ee459c9e97>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(domain, agent, epsilon_decay, n_episodes)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0;31m# agent.epsilon *= epsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode Number: {0}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-255-680fcc5f550b>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(domain, agent)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#: Learn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflashback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewstate\u001b[0m                            \u001b[0;31m#: Newstate becomes the current state for next iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-2046f9cf7cb6>\u001b[0m in \u001b[0;36mflashback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflashback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-2046f9cf7cb6>\u001b[0m in \u001b[0;36m_prep_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_sa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment(domain, agent, epsilon_decay, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
